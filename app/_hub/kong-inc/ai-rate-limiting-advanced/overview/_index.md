---
nav_title: Overview
---

The AI Rate Limiting Advanced plugin provides rate limiting for the providers used by any AI plugins. The
AI Rate Limiting plugin extends the
[Rate Limiting Advanced](/hub/kong-inc/rate-limiting-advanced/) plugin.

This token is using the tokens data returned by LLM provider to calculate the queries cose.
The same HTTP request can vary greatly in cost depending on the caluclation of the 
LLM providers.

A common pattern to protect your AI API is then to analyze and
assign costs to incoming  queries and rate limit the consumer's
cost for a given time window and providers.

You can also create a generic prompt rate limit using the request prompt provider (#request-prompt-function)

{:.note}
> **Notes:**
  * PostgreSQL 9.5+ is required when using the `cluster` strategy with `postgres` as the backing Kong cluster datastore.
  * The `dictionary_name` directive was added to prevent the usage of the `kong` shared dictionary, which could lead to `no memory` errors.
    * Known limitation: the cost for `AI proxy` will be only reflected during the next request.
    * Example: a request is made and `AI proxy` plugin is returning a token cost of `100` for the `OpenAi` provider:
      - The request will be made to OpenAi provider and response returnged to user
      - The next request will be blocked if rate limit is reached
    * Known limitation: the disable penalty will only work for the `requestPrompt` provider


## Headers sent to the client

When this plugin is enabled, Kong sends some additional headers back to the client
indicating the allowed limits, how many requests are available, and how long it will take
until the quota will be restored.

For example:

```plaintext
X-AI-RateLimit-Reset: 47
X-AI-RateLimit-Retry-After: 47
```

The plugin also sends headers indicating the limits in the time frame and the number
of remaining minutes for each providers:

```plaintext
X-AI-RateLimit-Limit-30-azure: 1000
X-AI-RateLimit-Remaining-30-azure: 950
```

If more than one limit is being set, the plugin returns a combination of more time limits:

```plaintext
X-AI-RateLimit-Limit-30-azure: 1000
X-AI-RateLimit-Remaining-30-azure: 950
X-AI-RateLimit-Limit-40-cohere: 2000
X-AI-RateLimit-Remaining-40-cohere: 1150
```

If any of the limits configured has been reached, the plugin returns an `HTTP/1.1 429` status
code to the client with the following JSON body:

```plaintext
{ "message": "API rate limit exceeded for provider azure, cohere" }
```

For each providers it will also indicate how long it  will take until the quota will be restored:

```plaintext
X-AI-RateLimit-Retry-After-30-azure: 1500
X-AI-RateLimit-Reset-30-azure: 1500
```

If using the request prompot provider, the plugin will send the query cost:

```plaintext
X-AI-RateLimit-Query-Cost: 100
```

The [`Retry-After`] headers will be present on `429` errors to indicate how long the service is
expected to be unavailable to the client. When using `window_type=sliding` and `RateLimit-Reset`, `Retry-After`
may increase due to the rate calculation for the sliding window.

{:.important}
You can optionally hide the cost, limit and remaining headers with the `hide_client_headers` option.

{:.important}
> The headers `RateLimit-Limit`, `RateLimit-Remaining`, and `RateLimit-Reset` are based on the Internet-Draft [RateLimit Header Fields for HTTP](https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers) and may change in the future to respect specification updates.


### Token count Strategies

The plugin supports three strategies to calulate the token:

| Strategy    | Description |
| --------- | ---- |
| `total_tokens`   | Represent the total number of tokens, including both the prompt and the generated completion, in the LLM's input sequence. 
| `prompt_tokens` | Represent the tokens provided by the user as input to the LLM, typically defining the context or task.
| `completion_tokens`   | Represent the tokens generated by the LLM in response to the prompt, representing the completed output or continuation of the task.


#### Request Prompt Function {#request-prompt-function}

You can decide to use a custom function to count the tokens for a requests. When using the request prompt provider, it will call the function to get the token count at the request level and implement a limit.


## Get started with the AI Rate Limiting Advanced plugin

* [Configuration reference](/hub/kong-inc/ai-rate-limiting-advanced/configuration/)
* [Basic configuration example](/hub/kong-inc/ai-rate-limiting-advanced/how-to/basic-example/)
* [Learn how to use the plugin](/hub/kong-inc/ai-rate-limiting-advanced/how-to/)

### Other AI plugins

You may also be interested in the following AI plugins:
* [AI Proxy](/hub/kong-inc/ai-proxy/)
* [AI Response Transformer](/hub/kong-inc/ai-request-transformer/)
* [AI Prompt Template](/hub/kong-inc/ai-prompt-template/)
* [AI Prompt Guard](/hub/kong-inc/ai-prompt-guard/)
* [AI Prompt Decorator](/hub/kong-inc/ai-prompt-decorator/)