---
name: Kafka Upstream
publisher: Kong Inc.
version: 0.4.x
desc: Transform requests into Kafka messages in a Kafka topic.
description: |
  This plugin transforms requests into [Kafka](https://kafka.apache.org/) messages
  in an [Apache Kafka](https://kafka.apache.org/) topic. For more information, see
  [Kafka topics](https://kafka.apache.org/documentation/#intro_concepts_and_terms).

  Kong also provides a Kafka Log plugin for publishing logs to a Kafka topic.
  See [Kafka Log](/hub/kong-inc/kafka-log/).
type: plugin
enterprise: true
plus: true
categories:
  - transformations
kong_version_compatibility:
  community_edition:
    compatible: null
  enterprise_edition:
    compatible:
      - 2.8.x
params:
  name: kafka-upstream
  dbless_compatible: 'yes'
  config:
    - name: bootstrap_servers
      required: true
      value_in_examples:
        BOOTSTRAP_SERVERS: null
      urlencode_in_examples: true
      default: null
      datatype: set of record elements
      description: |
        Set of bootstrap brokers in a `{host: host, port: port}` list format.
    - name: topic
      required: true
      value_in_examples:
        TOPIC: null
      urlencode_in_examples: true
      default: null
      datatype: string
      description: |
        The Kafka topic to publish to.
    - name: authentication.strategy
      required: false
      value_in_examples: sasl
      urlencode_in_examples: true
      default: null
      datatype: string
      description: |
        The authentication strategy for the plugin, the only option for the value is `sasl`.
    - name: authentication.mechanism
      required: false
      value_in_examples: PLAIN
      urlencode_in_examples: true
      default: null
      datatype: string
      description: |
        The SASL authentication mechanism, the two options for the value are: `PLAIN` and `SCRAM-SHA-256`.
    - name: authentication.user
      required: false
      value_in_examples: admin
      urlencode_in_examples: true
      default: null
      datatype: string
      encrypted: true
      description: |
        Username for SASL authentication.

        This field is _referenceable_, which means it can be securely stored as a
        [secret](/gateway/latest/plan-and-deploy/security/secrets-management/getting-started)
        in a vault. References must follow a [specific format](/gateway/latest/plan-and-deploy/security/secrets-management/reference-format).
    - name: authentication.password
      required: false
      value_in_examples: admin-secret
      urlencode_in_examples: true
      default: null
      datatype: string
      encrypted: true
      description: |
        Password for SASL authentication.

        This field is _referenceable_, which means it can be securely stored as a
        [secret](/gateway/latest/plan-and-deploy/security/secrets-management/getting-started)
        in a vault. References must follow a [specific format](/gateway/latest/plan-and-deploy/security/secrets-management/reference-format).
    - name: authentication.tokenauth
      required: false
      value_in_examples: false
      default: false
      datatype: boolean
      description: |
        Enable this to indicate `DelegationToken` authentication.
    - name: security.ssl
      required: false
      value_in_examples: false
      default: false
      datatype: boolean
      description: |
        Enables TLS.
    - name: security.certificate_id
      required: false
      urlencode_in_examples: true
      value_in_examples: nil
      default: null
      datatype: string
      description: |
        UUID of certificate entity for mTLS authentication.
    - name: timeout
      required: false
      default: '`10000`'
      value_in_examples: 10000
      datatype: integer
      description: |
        Socket timeout in milliseconds.
    - name: keepalive
      required: false
      default: '`60000`'
      value_in_examples: 60000
      datatype: integer
      description: |
        Keepalive timeout in milliseconds.
    - name: forward_method
      required: semi
      default: '`false`'
      datatype: boolean
      description: |
        Include the request method in the message. At least one of these must be true:
        `forward_method`, `forward_uri`, `forward_headers`, `forward_body`.
    - name: forward_uri
      required: semi
      default: '`false`'
      datatype: boolean
      description: |
        Include the request URI and URI arguments (as in, query arguments) in the message.
        At least one of these must be true: `forward_method`, `forward_uri`, `forward_headers`,
        `forward_body`.
    - name: forward_headers
      required: semi
      default: '`false`'
      datatype: boolean
      description: |
        Include the request headers in the message. At least one of these must be true:
        `forward_method`, `forward_uri`, `forward_headers`, `forward_body`.
    - name: forward_body
      required: semi
      default: '`true`'
      datatype: boolean
      description: |
        Include the request body in the message. At least one of these must be true:
        `forward_method`, `forward_uri`, `forward_headers`, `forward_body`.
    - name: cluster_name
      required: false
      default: '`<autogenerated-value>`'
      value_in_examples:
      datatype: string
      description: |
        An identifier for the Kafka cluster. By default, this field generates a
        random string. You can also set your own custom cluster identifier.

        If more than one Kafka plugin is configured without a `cluster_name`
        (that is, if the default autogenerated value is removed),
        these plugins will use the same producer, and by extension, the same
        cluster. Logs will be sent to the leader of the cluster.
    - name: producer_request_acks
      required: false
      default: '`1`'
      value_in_examples: -1
      datatype: integer
      description: |
        The number of acknowledgments the producer requires the leader to have received before
        considering a request complete. Allowed values: 0 for no acknowledgments; 1 for only the
        leader; and -1 for the full ISR (In-Sync Replica set).
    - name: producer_request_timeout
      required: false
      default: '`2000`'
      value_in_examples: 2000
      datatype: integer
      description: |
        Time to wait for a Produce response in milliseconds.
    - name: producer_request_limits_messages_per_request
      required: false
      default: '`200`'
      value_in_examples: 200
      datatype: integer
      description: |
        Maximum number of messages to include into a single Produce request.
    - name: producer_request_limits_bytes_per_request
      required: false
      default: '`1048576`'
      value_in_examples: 1048576
      datatype: integer
      description: |
        Maximum size of a Produce request in bytes.
    - name: producer_request_retries_max_attempts
      required: false
      default: '`10`'
      value_in_examples: 10
      datatype: integer
      description: |
        Maximum number of retry attempts per single Produce request.
    - name: producer_request_retries_backoff_timeout
      required: false
      default: '`100`'
      datatype: integer
      description: |
        Backoff interval between retry attempts in milliseconds.
    - name: producer_async
      required: false
      default: '`true`'
      datatype: boolean
      description: |
        Flag to enable asynchronous mode.
    - name: producer_async_flush_timeout
      required: false
      default: '`1000`'
      datatype: integer
      description: |
        Maximum time interval in milliseconds between buffer flushes in asynchronous mode.
    - name: producer_async_buffering_limits_messages_in_memory
      required: false
      default: '`50000`'
      datatype: integer
      description: |
        Maximum number of messages that can be buffered in memory in asynchronous mode.
---

### Enable on a service-less route

```bash
curl -X POST http://kong:8001/routes/my-route/plugins \
    --data "name=kafka-upstream" \
    --data "config.bootstrap_servers[1].host=localhost" \
    --data "config.bootstrap_servers[1].port=9092" \
    --data "config.topic=kong-upstream" \
    --data "config.timeout=10000" \
    --data "config.keepalive=60000" \
    --data "config.forward_method=false" \
    --data "config.forward_uri=false" \
    --data "config.forward_headers=false" \
    --data "config.forward_body=true" \
    --data "config.producer_request_acks=1" \
    --data "config.producer_request_timeout=2000" \
    --data "config.producer_request_limits_messages_per_request=200" \
    --data "config.producer_request_limits_bytes_per_request=1048576" \
    --data "config.producer_request_retries_max_attempts=10" \
    --data "config.producer_request_retries_backoff_timeout=100" \
    --data "config.producer_async=true" \
    --data "config.producer_async_flush_timeout=1000" \
    --data "config.producer_async_buffering_limits_messages_in_memory=50000"
```

## Implementation details

This plugin uses the [lua-resty-kafka](https://github.com/kong/lua-resty-kafka) client.

When encoding request bodies, several things happen:

* For requests with a content-type header of `application/x-www-form-urlencoded`, `multipart/form-data`,
  or `application/json`, this plugin passes the raw request body in the `body` attribute, and tries
  to return a parsed version of those arguments in `body_args`. If this parsing fails, an error message is
  returned and the message is not sent.
* If the `content-type` is not `text/plain`, `text/html`, `application/xml`, `text/xml`, or `application/soap+xml`,
  then the body will be base64-encoded to ensure that the message can be sent as JSON. In such a case,
  the message has an extra attribute called `body_base64` set to `true`.

## TLS

Enable TLS by setting `config.security.ssl` to `true`.

## mTLS

Enable mTLS by setting a valid UUID of a certificate in `config.security.certificate_id`.

Note that this option needs `config.security.ssl` set to true.
See [Certificate Object](https://docs.konghq.com/gateway/latest/admin-api/#certificate-object)
in the Admin API documentation for information on how to set up Certificates.

## SASL Authentication

To use SASL authentication, set the configuration option `config.authentication.strategy` to `sasl`.

Make sure that these mechanism are enabled on the Kafka side as well.

This plugin supports multiple authentication mechanisms including the following:

- **PLAIN:** Enable this mechanism by setting `config.authentication.mechanism` to `PLAIN`.
  You also need to provide a username and password with the config options `config.authentication.user`
  and `config.authentication.password` respectively.
- **SCRAM-SHA-256:** Enable this mechanism by setting `config.authentication.mechanism`
  to `SCRAM-SHA-256`. You also need to provide a username and password with the config options
  `config.authentication.user` and `config.authentication.password` respectively.
  {:.note}
  > In cryptography, the Salted Challenge Response Authentication Mechanism (SCRAM)
    is a family of modern, password-based challenge–response authentication mechanisms
    providing authentication of a user to a server.
- **Delegation Tokens:** Delegation Tokens can be generated in Kafka and then used to authenticate
  this plugin. `Delegation Tokens` leverage the `SCRAM-SHA-256` authentication mechanism. The `tokenID`
  is provided with the `config.authentication.user` field and the `token-hmac` is provided with the
  `config.authentication.password` field. To indicate that a token is used you have to set the
  `config.authentication.tokenauth` setting to `true`.

  [Read more on how to create, renew and revoke delegation tokens.](https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_delegation.html#authentication-using-delegation-tokens)

## Known issues and limitations

Known limitations:

1. Message compression is not supported.
2. The message format is not customizable.

## Quickstart

The following steps assume that {{site.base_gateway}} is installed and the Kafka Upstream plugin is enabled.

{:.note}
> **Note**: We use `zookeeper` in the following example, which is not required or has been removed on some Kafka versions. Refer to the [Kafka ZooKeeper documentation](https://kafka.apache.org/documentation/#zk) for more information.

1. Create a `kong-upstream` topic in your Kafka cluster:

    ```
    ${KAFKA_HOME}/bin/kafka-topics.sh --create \
        --zookeeper localhost:2181 \
        --replication-factor 1 \
        --partitions 10 \
        --topic kong-upstream
    ```

2. Create a Service-less Route, and add the `kafka-upstream` plugin to it:

    ```
    curl -X POST http://localhost:8001/routes \
        --data "name=kafka-upstream" \
        --data "hosts[]=kafka-upstream.dev"
    ```

    ```
    curl -X POST http://localhost:8001/routes/kafka-upstream/plugins \
        --data "name=kafka-upstream" \
        --data "config.bootstrap_servers[1].host=localhost" \
        --data "config.bootstrap_servers[1].port=9092" \
        --data "config.topic=kong-upstream"
    ```

3. In a different console, start a Kafka consumer:

    ```
    ${KAFKA_HOME}/bin/kafka-console-consumer.sh \
        --bootstrap-server localhost:9092 \
        --topic kong-upstream \
        --partition 0 \
        --from-beginning \
        --timeout-ms 1000
    ```

4. Make sample requests:

    ```
    curl -X POST http://localhost:8000 --header 'Host: kafka-upstream.dev' foo=bar
    ```

    You should receive a `200 { message: "message sent" }` response, and should see the request bodies appear on
    the Kafka consumer console you started in the previous step.

---

## Changelog

### Kong Gateway 2.8.x (plugin version 0.4.0)

* Added the `cluster_name` configuration parameter.

* The `authentication.user` and `authentication.password` configuration fields are now marked as
referenceable, which means they can be securely stored as
[secrets](/gateway/latest/plan-and-deploy/security/secrets-management/getting-started)
in a vault. References must follow a [specific format](/gateway/latest/plan-and-deploy/security/secrets-management/reference-format).

### Kong Gateway 2.7.x (plugin version 0.3.0)

* Starting with {{site.base_gateway}} 2.7.0.0, if keyring encryption is enabled,
 the `config.authentication.user` and `config.authentication.password` parameter
 values will be encrypted.

   {:.important}
   > There's a bug in {{site.base_gateway}} that prevents keyring encryption
   from working on deeply nested fields, so the `encrypted=true` setting does not
   currently have any effect in this plugin.
