description: |
  This configuration enables AI Proxy with Llama2.

prereqs: 
  - A running Llama2 instance.
  - A route with `paths[]=~/llama2-chat$` configured. Apply the plugin to this route.

config:
  route_type: "llm/v1/chat"
  model:
    provider: "llama2"
    name: "llama2"
    options:
      llama2_format: "ollama"
      upstream_url: "http://llama2-server.local:11434/api/chat"
